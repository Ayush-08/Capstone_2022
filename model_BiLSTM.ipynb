{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ta1kUvoLEWKK"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"../content/drive/MyDrive/test.zip\",\"r\") as z:\n",
        "    z.extractall(\".\")\n",
        "with zipfile.ZipFile(\"../content/drive/MyDrive/train.zip\",\"r\") as z:\n",
        "    z.extractall(\".\")"
      ],
      "metadata": {
        "id": "C3J-KY4BEYRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pywt\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from glob import glob\n",
        "import scipy\n",
        "from scipy.signal import butter, lfilter, convolve, boxcar\n",
        "from scipy.signal import freqz\n",
        "from scipy.fftpack import fft, ifft\n",
        "import os\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def wavelet_denoising(x, wavelet='db2', level=3):\n",
        "    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n",
        "    sigma = (1/0.6745) * madev(coeff[-level])\n",
        "    uthresh = sigma * np.sqrt(2 * np.log(len(x)))\n",
        "    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n",
        "    return pywt.waverec(coeff, wavelet, mode='per')\n",
        "def madev(d, axis=None):\n",
        "    \"\"\" Mean absolute deviation of a signal \"\"\"\n",
        "    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n",
        "#############function to read data###########\n",
        "\n",
        "def prepare_data_train(fname):\n",
        "    \"\"\" read and prepare training data \"\"\"\n",
        "    # Read data\n",
        "    data = pd.read_csv(fname)\n",
        "    # events file\n",
        "    events_fname = fname.replace('_data','_events')\n",
        "    # read event file\n",
        "    labels= pd.read_csv(events_fname)\n",
        "    clean=data.drop(['id' ], axis=1)#remove id\n",
        "    labels=labels.drop(['id' ], axis=1)#remove id\n",
        "    return  clean,labels\n",
        "def prepare_data_test(fname):\n",
        "    \"\"\" read and prepare test data \"\"\"\n",
        "    # Read data\n",
        "    data = pd.read_csv(fname)\n",
        "    return data\n",
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "scaler= StandardScaler()\n",
        "def data_preprocess_train(X):\n",
        "    X_prep=scaler.fit_transform(X)\n",
        "    #do here your preprocessing\n",
        "    return X_prep\n",
        "def data_preprocess_test(X):\n",
        "    X_prep=scaler.transform(X)\n",
        "    #do here your preprocessing\n",
        "    return X_prep\n",
        "subjects = range(1,6)\n",
        "     \n"
      ],
      "metadata": {
        "id": "BPnvp8WKEa1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "import pandas as pd\n",
        "ids_tot = []\n",
        "pred_tot = []\n",
        "X_train_butter = []\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as  np\n",
        "\n",
        "###loop on subjects and 8 series for train data + 2 series for test data\n",
        "y_raw= []\n",
        "raw = []\n",
        "y_rawt= []\n",
        "rawt = []\n",
        "for subject in subjects:\n",
        "    \n",
        "    ################ READ DATA ################################################\n",
        "    fnames =  sorted(glob('train/subj%d_series*_data.csv' % (subject)))\n",
        "\n",
        "\n",
        "#    fnames =  glob('../input/train/subj1_series1_events.csv')\n",
        "#    fnames =  glob('../input/train/subj1_series1_data.csv')\n",
        "    for fname in fnames:\n",
        "      data,labels=prepare_data_train(fname)\n",
        "      raw.append(data)\n",
        "      y_raw.append(labels)\n",
        "\n",
        "    for fname in fnames:\n",
        "      with open(fname) as myfile:\n",
        "        head = [next(myfile) for x in range(10)]\n",
        "        \n",
        "\n",
        "      \n",
        "        \n",
        "X = pd.concat(raw)\n",
        "y = pd.concat(y_raw)\n",
        "    #transform in numpy array\n",
        "    #transform train data in numpy array\n",
        "X_train =np.asarray(X.astype(float))\n",
        "y_train = np.asarray(y.astype(float))\n",
        "from sklearn.preprocessing import StandardScaler,Normalizer,MinMaxScaler\n",
        "scaler= StandardScaler()\n",
        "def data_preprocess_train(X):\n",
        "    X_prep=scaler.fit_transform(X)\n",
        "    #do here your preprocessing\n",
        "    return X_prep\n",
        "fs = 500.0\n",
        "lowcut = 7.0\n",
        "highcut = 30.0\n",
        "\n",
        "\n",
        "x_train_butter=wavelet_denoising(X_train)\n",
        "x_train=data_preprocess_train(x_train_butter)\n",
        "splitrate=-x_train.shape[0]//5*2\n",
        "xval=x_train[splitrate:splitrate//2]\n",
        "yval=y_train[splitrate:splitrate//2]\n",
        "xtest=x_train[splitrate//2:]\n",
        "ytest=y_train[splitrate//2:]\n",
        "xtrain=x_train[:splitrate]\n",
        "ytrain=y_train[:splitrate]\n",
        "     \n"
      ],
      "metadata": {
        "id": "CpNvcJ02Egm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pywt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "def wavelet_denoising(x, wavelet='db2', level=3):\n",
        "    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n",
        "    sigma = (1/0.6745) * madev(coeff[-level])\n",
        "    uthresh = sigma * np.sqrt(2 * np.log(len(x)))\n",
        "    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n",
        "    return pywt.waverec(coeff, wavelet, mode='per')\n",
        "def madev(d, axis=None):\n",
        "    \"\"\" Mean absolute deviation of a signal \"\"\"\n",
        "    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n",
        "\n",
        "signal=pd.read_csv('train/subj1_series1_data.csv')\n",
        "signal = signal.drop(\"id\", axis=1)\n",
        "filtered = wavelet_denoising(signal, wavelet='db2', level=3)\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(figsize=(8,5))\n",
        "ax.plot(signal.iloc[:10000,1], label='signal', color=\"b\", alpha=0.5,)\n",
        "ax.plot(filtered[:10000,1], label='reconstructed signal',color=\"k\")\n",
        "ax.legend(loc='upper left')\n",
        "ax.set_title('Denoising with DWT')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UIiyy8CRElQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "ye=pd.DataFrame(y_train)\n",
        "\n",
        "ye.columns=[\"Handstart \",\"Grasping \",\"Lift \",\"Hold \",\"Replace \",\"Release\"]\n",
        "categories = list(ye.columns.values)\n",
        "sns.set(font_scale = 1)\n",
        "plt.figure(figsize=(15,8))\n",
        "ax= sns.barplot(categories, ye.iloc[:,0:].sum().values)\n",
        "plt.title(\"Number of samples labeled as active (1) out of {0} length data\".format((ye.shape[0])),fontsize=20)\n",
        "\n",
        "plt.ylabel('Number of events', fontsize=18)\n",
        "plt.xlabel('Event Type ', fontsize=12)\n",
        "#adding the text labels\n",
        "rects = ax.patches\n",
        "labels = ye.iloc[:,0:].sum().values\n",
        "\n",
        "for rect, label in zip(rects, labels):\n",
        "    height = rect.get_height()\n",
        "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom', fontsize=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "66N2VDgcEoLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import keras\n",
        "from keras import regularizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LSTM, BatchNormalization, Conv2D, Flatten, MaxPooling2D, Dropout, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "#from keras.optimizers import Adam\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,roc_curve,auc\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "ihBgaaYBEq2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import keras\n",
        "from keras import regularizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LSTM, BatchNormalization, Conv2D, Flatten, MaxPooling2D, Dropout, Activation, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "#from keras.optimizers import Adam\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,roc_curve,auc\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "ByNZ76ROFsDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "time_steps = 1000\n",
        "subsample = 10\n",
        "model3 = Sequential()\n",
        "model3.add(Bidirectional(LSTM(64,  return_sequences=True), input_shape = (time_steps//subsample, 32)))\n",
        "#model3.add(Dropout(0.5))\n",
        "model3.add(Bidirectional(LSTM(64, return_sequences=True), input_shape = (time_steps//subsample, 32)))\n",
        "#model3.add(Dropout(0.5))\n",
        "model3.add(LSTM(64,input_shape = (time_steps//subsample, 32), return_sequences=True))\n",
        "#model3.add(Dropout(0.5))\n",
        "model3.add(Bidirectional(LSTM(64)))\n",
        "model3.add(Dense(6, activation='sigmoid'))\n",
        "model3.compile(loss='binary_crossentropy', optimizer= Adam(lr = 0.0001), metrics=['accuracy'])\n",
        "model3.summary()\n",
        "     "
      ],
      "metadata": {
        "id": "P30hSc2OEtqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AyiXjpKsFUnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7erRr2FzFVbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def valgenerator():\n",
        "    while 1:\n",
        "        batch_size=32\n",
        "        x_time_data = np.zeros((batch_size, time_steps//subsample, 32))\n",
        "        yy = []\n",
        "        for i in range(batch_size):\n",
        "            random_index = np.random.randint(0, len(xval)-time_steps)\n",
        "            x_time_data[i] = xval[random_index:random_index+time_steps:subsample]\n",
        "            yy.append(yval[random_index + time_steps])\n",
        "        yy = np.asarray(yy)\n",
        "        yield x_time_data, yy\n",
        "     "
      ],
      "metadata": {
        "id": "nxNz9EcIEyPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start=time.time()\n",
        "def generator(batch_size):\n",
        "    while 1:\n",
        "        \n",
        "        x_time_data = np.zeros((batch_size, time_steps//subsample, 32))\n",
        "        yy = []\n",
        "        for i in range(batch_size):\n",
        "            random_index = np.random.randint(0, len(xtrain)-time_steps)\n",
        "            x_time_data[i] = xtrain[random_index:random_index+time_steps:subsample]\n",
        "            yy.append(ytrain[random_index + time_steps])\n",
        "        yy = np.asarray(yy)\n",
        "        yield x_time_data, yy\n",
        "\n",
        "history =model.fit_generator(generator(32), steps_per_epoch = 200, epochs = 50,validation_data=valgenerator(), validation_steps=100)\n",
        "print('training time taken: ',round(time.time()-start,0),'seconds')"
      ],
      "metadata": {
        "id": "07QExCjjFFU5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}